{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../input/beeVSwasp/')\n",
    "BEE_DIR = Path('../input/beeVSwasp/bee/')\n",
    "WASP_DIR = Path('../input/beeVSwasp/wasp//')\n",
    "\n",
    "bee_files = sorted(list(BEE_DIR.rglob('*.jpg')))\n",
    "bee_files = [str(x) for x in bee_files]\n",
    "\n",
    "wasp_files = sorted(list(WASP_DIR.rglob('*.jpg')))\n",
    "wasp_files = [str(x) for x in wasp_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2469 2126\n"
     ]
    }
   ],
   "source": [
    "BEE_SIZE = len(bee_files)\n",
    "WASP_SIZE = len(wasp_files)\n",
    "print(len(bee_files), len(wasp_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [SCALE_SIZE, SCALE_SIZE])\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 == Bee\n",
    "\n",
    "0 == Wasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1 for _ in range(BEE_SIZE)] + [0 for _ in range(WASP_SIZE)]\n",
    "files = bee_files + wasp_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds = tf.data.Dataset.from_tensor_slices(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Установка размера буфера перемешивания, равного набору данных, гарантирует\n",
    "# полное перемешивание данных.\n",
    "ds = image_label_ds.shuffle(buffer_size=BEE_SIZE+WASP_SIZE)\n",
    "ds = ds.repeat()\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "# `prefetch` позволяет датасету извлекать пакеты в фоновом режиме, во время обучения модели.\n",
    "ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-1e14ab75f0fd>:2: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "ds = image_label_ds.apply(\n",
    "  tf.data.experimental.shuffle_and_repeat(buffer_size=BEE_SIZE+WASP_SIZE))\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "ds = ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu_alpha = 0.2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "def conv2d( inputs , filters , stride_size ):\n",
    "    out = tf.nn.conv2d( inputs , filters , strides=[ 1 , stride_size , stride_size , 1 ] , padding='VALID' ) \n",
    "    return tf.nn.leaky_relu( out , alpha=leaky_relu_alpha ) \n",
    "\n",
    "def maxpool( inputs , pool_size , stride_size ):\n",
    "    return tf.nn.max_pool2d( inputs , ksize=[ 1 , pool_size , pool_size , 1 ] , padding='VALID' , strides=[ 1 , stride_size , stride_size , 1 ] )\n",
    "\n",
    "def dense( inputs , weights ):\n",
    "    x = tf.nn.leaky_relu( tf.matmul( inputs , weights ) , alpha=leaky_relu_alpha )\n",
    "    return tf.nn.dropout( x , rate=dropout_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.initializers.glorot_uniform()\n",
    "def get_weight( shape , name ):\n",
    "    return tf.Variable( initializer( shape ) , name=name , trainable=True , dtype=tf.float32 )\n",
    "\n",
    "shapes = [\n",
    "    [ 3 , 3 , 3 , 16 ] , \n",
    "    [ 3 , 3 , 16 , 16 ] , \n",
    "    [ 3 , 3 , 16 , 32 ] , \n",
    "    [ 3 , 3 , 32 , 32 ] ,\n",
    "    [ 3 , 3 , 32 , 64 ] , \n",
    "    [ 3 , 3 , 64 , 64 ] ,\n",
    "    [ 3 , 3 , 64 , 128 ] , \n",
    "    [ 3 , 3 , 128 , 128 ] ,\n",
    "    [ 3 , 3 , 128 , 256 ] , \n",
    "    [ 3 , 3 , 256 , 256 ] ,\n",
    "    [ 3 , 3 , 256 , 512 ] , \n",
    "    [ 3 , 3 , 512 , 512 ] ,\n",
    "    [ 8192 , 3600 ] , \n",
    "    [ 3600 , 2400 ] ,\n",
    "    [ 2400 , 1600 ] , \n",
    "    [ 1600 , 800 ] ,\n",
    "    [ 800 , 64 ] ,\n",
    "    [ 64 , 2 ] ,\n",
    "]\n",
    "\n",
    "weights = []\n",
    "for i in range( len( shapes ) ):\n",
    "    weights.append( get_weight( shapes[ i ] , 'weight{}'.format( i ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model( x ) :\n",
    "    x = tf.cast( x , dtype=tf.float32 )\n",
    "    c1 = conv2d( x , weights[ 0 ] , stride_size=1 ) \n",
    "    c1 = conv2d( c1 , weights[ 1 ] , stride_size=1 ) \n",
    "    p1 = maxpool( c1 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c2 = conv2d( p1 , weights[ 2 ] , stride_size=1 )\n",
    "    c2 = conv2d( c2 , weights[ 3 ] , stride_size=1 ) \n",
    "    p2 = maxpool( c2 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c3 = conv2d( p2 , weights[ 4 ] , stride_size=1 ) \n",
    "    c3 = conv2d( c3 , weights[ 5 ] , stride_size=1 ) \n",
    "    p3 = maxpool( c3 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c4 = conv2d( p3 , weights[ 6 ] , stride_size=1 )\n",
    "    c4 = conv2d( c4 , weights[ 7 ] , stride_size=1 )\n",
    "    p4 = maxpool( c4 , pool_size=2 , stride_size=2 )\n",
    "\n",
    "    c5 = conv2d( p4 , weights[ 8 ] , stride_size=1 )\n",
    "    c5 = conv2d( c5 , weights[ 9 ] , stride_size=1 )\n",
    "    p5 = maxpool( c5 , pool_size=2 , stride_size=2 )\n",
    "\n",
    "    c6 = conv2d( p5 , weights[ 10 ] , stride_size=1 )\n",
    "    c6 = conv2d( c6 , weights[ 11 ] , stride_size=1 )\n",
    "    p6 = maxpool( c6 , pool_size=2 , stride_size=2 )\n",
    "\n",
    "    flatten = tf.reshape( p6 , shape=( tf.shape( p6 )[0] , -1 ))\n",
    "\n",
    "    d1 = dense( flatten , weights[ 12 ] )\n",
    "    d2 = dense( d1 , weights[ 13 ] )\n",
    "    d3 = dense( d2 , weights[ 14 ] )\n",
    "    d4 = dense( d3 , weights[ 15 ] )\n",
    "    d5 = dense( d4 , weights[ 16 ] )\n",
    "    logits = tf.matmul( d5 , weights[ 17 ] )\n",
    "\n",
    "    return tf.nn.softmax( logits )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.reshape(img, [1,224,224,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = get_weight([3, 3, 3, 16], 'w_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 222, 222, 16), dtype=float32, numpy=\n",
       "array([[[[-0.52047575,  0.6271066 ,  0.25165382, ..., -0.05929473,\n",
       "          -0.83652955,  0.29842576],\n",
       "         [-0.5227873 ,  0.62936515,  0.2514286 , ..., -0.06060681,\n",
       "          -0.83942133,  0.2986972 ],\n",
       "         [-0.5242499 ,  0.6304587 ,  0.2515047 , ..., -0.06021832,\n",
       "          -0.8430604 ,  0.2998769 ],\n",
       "         ...,\n",
       "         [-0.4301899 ,  0.577885  ,  0.21271932, ..., -0.05665542,\n",
       "          -0.7314661 ,  0.2536858 ],\n",
       "         [-0.44182697,  0.58431804,  0.21563528, ..., -0.05855185,\n",
       "          -0.7430385 ,  0.25769994],\n",
       "         [-0.44997513,  0.58851004,  0.21680366, ..., -0.05814422,\n",
       "          -0.7553606 ,  0.26206836]],\n",
       "\n",
       "        [[-0.5182987 ,  0.624975  ,  0.25152117, ..., -0.05818411,\n",
       "          -0.8350739 ,  0.29731023],\n",
       "         [-0.5197311 ,  0.6259663 ,  0.2501934 , ..., -0.05969538,\n",
       "          -0.8354109 ,  0.29761684],\n",
       "         [-0.5211841 ,  0.6267526 ,  0.25069365, ..., -0.06135386,\n",
       "          -0.8385257 ,  0.2983462 ],\n",
       "         ...,\n",
       "         [-0.4391498 ,  0.5833875 ,  0.2141773 , ..., -0.05628761,\n",
       "          -0.7437401 ,  0.25869855],\n",
       "         [-0.44756937,  0.58870566,  0.21609077, ..., -0.059198  ,\n",
       "          -0.75204843,  0.26122272],\n",
       "         [-0.45653626,  0.5905545 ,  0.21781231, ..., -0.06004821,\n",
       "          -0.76292384,  0.26534757]],\n",
       "\n",
       "        [[-0.51628494,  0.6234645 ,  0.25162965, ..., -0.05855891,\n",
       "          -0.8357656 ,  0.29738653],\n",
       "         [-0.5174725 ,  0.62454194,  0.2517717 , ..., -0.0579625 ,\n",
       "          -0.8363633 ,  0.29866385],\n",
       "         [-0.5190981 ,  0.62444264,  0.2520903 , ..., -0.05979406,\n",
       "          -0.8374023 ,  0.29780883],\n",
       "         ...,\n",
       "         [-0.4460082 ,  0.58862007,  0.21611871, ..., -0.05734142,\n",
       "          -0.75527745,  0.26327327],\n",
       "         [-0.45306212,  0.5926854 ,  0.21939503, ..., -0.06073905,\n",
       "          -0.76373404,  0.26567283],\n",
       "         [-0.4597584 ,  0.5933371 ,  0.22229469, ..., -0.05814736,\n",
       "          -0.773145  ,  0.2676701 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.48977017,  0.574382  ,  0.23503742, ..., -0.05793896,\n",
       "          -0.77714336,  0.27765322],\n",
       "         [-0.49305847,  0.57954186,  0.23682503, ..., -0.05803231,\n",
       "          -0.7833339 ,  0.27992633],\n",
       "         [-0.4950914 ,  0.58461064,  0.23702854, ..., -0.05658519,\n",
       "          -0.7909972 ,  0.28202784],\n",
       "         ...,\n",
       "         [-0.5051798 ,  0.6137839 ,  0.24668486, ..., -0.05517727,\n",
       "          -0.82468617,  0.29457036],\n",
       "         [-0.50442266,  0.62040144,  0.24934539, ..., -0.05404109,\n",
       "          -0.827083  ,  0.29542896],\n",
       "         [-0.5030979 ,  0.62683356,  0.25061065, ..., -0.05505071,\n",
       "          -0.827303  ,  0.29535735]],\n",
       "\n",
       "        [[-0.4892105 ,  0.57494766,  0.23600155, ..., -0.05860952,\n",
       "          -0.7763425 ,  0.2768691 ],\n",
       "         [-0.4904846 ,  0.58013713,  0.23732449, ..., -0.05744612,\n",
       "          -0.78349745,  0.28060135],\n",
       "         [-0.4932711 ,  0.5867914 ,  0.23773667, ..., -0.05649368,\n",
       "          -0.79040104,  0.28325042],\n",
       "         ...,\n",
       "         [-0.503919  ,  0.61571145,  0.24654737, ..., -0.05490633,\n",
       "          -0.8227647 ,  0.29534018],\n",
       "         [-0.50151587,  0.6214174 ,  0.24753179, ..., -0.05549822,\n",
       "          -0.824383  ,  0.29361987],\n",
       "         [-0.500764  ,  0.62708914,  0.2496924 , ..., -0.05490316,\n",
       "          -0.8286466 ,  0.29521877]],\n",
       "\n",
       "        [[-0.48902756,  0.5747339 ,  0.23498745, ..., -0.05915118,\n",
       "          -0.77627355,  0.27742174],\n",
       "         [-0.4894195 ,  0.57976997,  0.2369006 , ..., -0.0579389 ,\n",
       "          -0.7837963 ,  0.28027204],\n",
       "         [-0.4920254 ,  0.5867608 ,  0.23787974, ..., -0.05583559,\n",
       "          -0.7903241 ,  0.2830168 ],\n",
       "         ...,\n",
       "         [-0.50312287,  0.61863595,  0.24661498, ..., -0.05104547,\n",
       "          -0.8218956 ,  0.29432508],\n",
       "         [-0.50239694,  0.6274305 ,  0.24703956, ..., -0.05408784,\n",
       "          -0.8259459 ,  0.2956215 ],\n",
       "         [-0.50341177,  0.63456416,  0.24769694, ..., -0.05471604,\n",
       "          -0.8312437 ,  0.2948246 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.conv2d(img, W, strides=[1, 1, 1, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
